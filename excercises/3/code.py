from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.appName("SparkPractice").getOrCreate()

data = [
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
    ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2)
]

df = spark.createDataFrame(data, ["column0", "column1", "column2", "label"])

result = df.withColumn("count", expr("count(*) over (partition by column0, column1, column2)"))
result.show(truncate=False)
